{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import textract\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function uses PyPDFLoader to load a PDF file from the specified path and splits it into pages. It's useful for processing PDFs page by page.\n",
    "def load_and_split_pdf(file_path):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function extracts text from a PDF file using textract, saves it to a text file, and then reads it back into a string. It's an alternate method for handling PDFs, especially when dealing with large text content.\n",
    "def process_text_from_pdf(file_path):\n",
    "    doc = textract.process(file_path)\n",
    "    with open('polish-recipes.txt', 'w') as f:\n",
    "        f.write(doc.decode('utf-8'))\n",
    "    with open('polish-recipes.txt', 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function uses GPT2TokenizerFast to tokenize the given text and returns the number of tokens. It's important for understanding the size of the text in terms of tokens, which is a common measure in NLP.\n",
    "def count_tokens(text):\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "    return len(tokenizer.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, the text is split into smaller chunks using a RecursiveCharacterTextSplitter. This is useful for processing large texts in manageable sizes, especially when working with language models.\n",
    "def split_text(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=24, length_function=count_tokens)\n",
    "    return text_splitter.create_documents([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function visualizes the distribution of token counts across the chunks of text. It helps in understanding how the text is distributed and is useful for debugging and analysis.\n",
    "def visualize_data(chunks):\n",
    "    token_counts = [count_tokens(chunk.page_content) for chunk in chunks]\n",
    "    df = pd.DataFrame({'Token Count': token_counts})\n",
    "    df.hist(bins=19)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function generates embeddings for each text chunk using OpenAIEmbeddings and stores them in a FAISS vector database. It's crucial for efficient similarity searches in large text corpora.\n",
    "def create_embeddings_and_database(chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function initializes and displays an interactive chatbot interface within a Jupyter environment. It utilizes a conversational retrieval chain to generate responses based on the user's input and the document's content.\n",
    "def create_chatbot_interface(db):\n",
    "    qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1), db.as_retriever())\n",
    "    chat_history = []\n",
    "\n",
    "    def on_submit(change):\n",
    "        query = input_box.value\n",
    "        input_box.value = \"\"\n",
    "\n",
    "        if query.lower() == 'exit':\n",
    "            print(\"Exiting chatbot.\")\n",
    "            return\n",
    "\n",
    "        result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "        chat_history.append((query, result['answer']))\n",
    "        display(widgets.HTML(f'<b>User:</b> {query}'))\n",
    "        display(widgets.HTML(f'<b><font color=\"blue\">Chatbot:</font></b> {result[\"answer\"]}'))\n",
    "\n",
    "    input_box = widgets.Text(placeholder='Enter your question:')\n",
    "    input_box.on_submit(on_submit)\n",
    "    display(input_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section is the entry point of your script. It orchestrates the loading of the PDF, processing the text, and starting the chatbot interface.\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your OpenAI API key as an environment variable for security\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "\n",
    "    # Path to your PDF file\n",
    "    pdf_file_path = \"./your-pdf-file.pdf\"\n",
    "\n",
    "    # Choose either simple or advanced method for processing the PDF\n",
    "    # Simple Method: Load and split the PDF\n",
    "    pages = load_and_split_pdf(pdf_file_path)\n",
    "\n",
    "    # Advanced Method: Process text from PDF and split into chunks\n",
    "    # Uncomment these lines if you want to use the advanced method\n",
    "    # text = process_text_from_pdf(pdf_file_path)\n",
    "    # chunks = split_text(text)\n",
    "\n",
    "    # Optional: Visualize data\n",
    "    # Uncomment to visualize data\n",
    "    # visualize_data(chunks)\n",
    "\n",
    "    # Create embeddings and database from chunks\n",
    "    # Uncomment and modify these lines if you're using the advanced method\n",
    "    # db = create_embeddings_and_database(chunks)\n",
    "\n",
    "    # Start the chatbot interface\n",
    "    # Uncomment and modify this line if you're using the advanced method\n",
    "    # create_chatbot_interface(db)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
